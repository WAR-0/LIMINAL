# Work-Stealing Deques and Fair Scheduling in Cilk vs. Java Fork/Join

## Introduction

Dynamic fork–join frameworks like **MIT Cilk** and Java’s **ForkJoinPool** achieve load-balanced parallel execution *without a central scheduler* by employing **work-stealing**. In a work-stealing scheduler, each worker thread owns a double-ended queue (deque) of tasks. Workers **execute tasks from one end (LIFO for local efficiency)** and **steal tasks from the opposite end of other workers’ deques (FIFO from the thief’s perspective)**[\[1\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=these%20in%20a%20random%20order,form%20of%20hierarchy%20without%20balancing). This decentralized design keeps all processors busy: any thread that runs out of work randomly searches for a victim and steals a task[\[2\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Fork%2Fjoin%20is%20different%20from%20a,From%20Fork%2FJoin). The result is near-optimal load balancing in theory – Blumofe et al.’s seminal work-stealing algorithm guarantees an expected execution time of *T\<sub\>1\</sub\>/P \+ O(T\<sub\>∞\</sub\>)* on *P* processors (where *T\<sub\>1\</sub\>* is serial work and *T\<sub\>∞\</sub\>* is the critical-path length)[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with)[\[4\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=contention%20that%20negatively%20impact%20performance,such%20as%20those%20built%20with). In practice, this means each core gets roughly an equal share of work without idle time, even if tasks spawn dynamically or unevenly[\[2\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Fork%2Fjoin%20is%20different%20from%20a,From%20Fork%2FJoin). Crucially, no global dispatcher is needed – **load balancing emerges from local deques and random steals**, avoiding the bottleneck of a central queue[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task). This report examines how Cilk and Java ForkJoinPool implement work-stealing deques and scheduling policies, focusing on *fairness* and *dynamic load balance* in agent-like task workloads. We compare **Cilk’s “work-first” scheduling** (a hallmark of Cilk’s design) against alternative **“help-first”** strategies, and discuss how each system measures and attains fairness across workers and tasks. Key implementation details – such as deque design, steal algorithms, and adaptive heuristics – are covered, along with any available metrics on load-balance efficiency and fairness.

## Work-Stealing Deque Design

Both Cilk and ForkJoinPool rely on a **lock-free work-stealing deque** per worker thread to manage tasks efficiently. A worker pushes new tasks onto one end of its deque and pops tasks from the same end (treating it like a stack for its local tasks), which enables LIFO execution that keeps recent data in cache for better locality[\[1\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=these%20in%20a%20random%20order,form%20of%20hierarchy%20without%20balancing). Thieves steal from the opposite end (the “front” of the deque) using an atomic operation, taking the oldest enqueued task. This design minimizes contention: a worker and a thief operate on opposite ends, and at most one thief contends for the front at a time. A well-known implementation is the **Chase–Lev deque**, which many frameworks adopt for speed and simplicity. In such a deque, the owner can push/pop the **bottom** with only minimal fencing, while a thief performs an atomic Compare-and-Swap on the **top** index to claim a task[\[6\]](https://hackage.haskell.org/package/chaselev-deque#:~:text=chaselev,stealing)[\[7\]](https://blog.molecular-matters.com/2015/09/25/job-system-2-0-lock-free-work-stealing-part-3-going-lock-free/#:~:text=Musings%20blog.molecular,Steals%20a%20job). Cilk’s earlier implementations (e.g. Cilk-5) used the “THE protocol,” a deque design where pushing is lock-free and stealing uses a lock on the victim’s deque[\[8\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=to%20the%20THE%20protocol%20,If%20the%20asyncs%20are%20scheduled)[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task). Java’s ForkJoinPool uses a similar circular array deque algorithm derived from work-stealing research (a variant of Chase–Lev). In Java’s case, each ForkJoinWorkerThread maintains a deque (WorkQueue), and idle threads scan for non-empty deques to steal tasks. The *deque structure is crucial for fairness and efficiency*: it ensures local threads get tasks in the order that maximizes cache-hit and execution efficiency (usually LIFO), while stolen tasks are taken in roughly FIFO order from a victim, helping distribute older backlog work to idling workers[\[1\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=these%20in%20a%20random%20order,form%20of%20hierarchy%20without%20balancing).

**Steal Strategy:** Both Cilk and ForkJoinPool use *randomized victim selection* for steals[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with)[\[4\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=contention%20that%20negatively%20impact%20performance,such%20as%20those%20built%20with). When a thread becomes idle, it randomly picks another worker’s deque to probe. Random stealing avoids hot spots and leads to probabilistic load balance: with high probability, all workers get a similar amount of work over time[\[2\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Fork%2Fjoin%20is%20different%20from%20a,From%20Fork%2FJoin). This stochastic fairness means no single thread becomes a long-term bottleneck. Some implementations use a round-robin or other pattern, but random selection (as in Cilk’s algorithm) has provable efficiency and low contention on average[\[9\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=the%20stealing%20phase%20when%20an,on%20P%20processors%2C%20and%20also). Notably, the cost of unsuccessful steals is kept low (often just a failed atomic check), and successful steals involve transferring a task but no global coordination.

**Adaptive Heuristics:** Modern work-stealing runtimes include heuristics to improve performance and fairness in edge cases. For example, tasks that are very small can incur overhead if stolen excessively, so frameworks often incorporate *“chunking”* or thresholding to reduce task granularity. Cilk’s runtime uses **lazy task creation** (also called “spawn elision”) to avoid creating tons of tiny tasks – the compiler may execute small spawns sequentially to reduce overhead[\[10\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=predicted%20by%20past%20work%20%28e,). In the Java Fork/Join framework, a method getSurplusQueuedTaskCount() lets a task check how much work is in its local deque; sample fork/join code uses this to decide when to split further or compute directly[\[11\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=%7D%20if%20%28n%20,2%29%3B%20%7D). This adaptive strategy ensures that if a worker already has a *surplus* of pending tasks (meaning plenty of work for others to steal), it will hold off generating more fine-grained tasks, thereby balancing overhead vs. parallelism. Another ForkJoinPool feature is the **ManagedBlocker** API, which can temporarily boost the pool size if threads block on I/O or locks[\[12\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=However%20there%20is%20a%20trap%3A,s). This prevents throughput loss or unfairness when some workers are idle only due to blocking; new threads are spawned to maintain parallelism, then removed when not needed[\[12\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=However%20there%20is%20a%20trap%3A,s).

## Cilk’s Work-First Scheduling Principle

**Cilk** pioneered a scheduling policy known as the **work-first principle**. In essence, **work-first scheduling means “steal the continuation, not the new child.”** When a Cilk function spawns a new parallel sub-task, the spawning worker **immediately begins executing the spawned child task,** treating it as the next work to do, and pushes the remainder of the parent function (the *continuation* after the spawn) onto its deque to be stolen by others[\[13\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=4%20Work,help%20from%20its%20peer%20workers). The child runs in the current thread (reusing the call stack frame, etc.), while the parent’s continuation becomes a suspended task that peers can grab. By doing this, Cilk minimizes overhead on the thread’s *current* execution path – it executes the new task using the current call stack (“fast clone”), and only incurs the cost of creating a deque entry for the continuation (“slow clone”)[\[14\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=The%20work,overhead%20than%20the%20slow%20clone). **The rationale** is that steals are assumed to be relatively infrequent (only when load imbalance occurs)[\[14\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=The%20work,overhead%20than%20the%20slow%20clone). Therefore, the work-first policy optimizes for the common case: as long as no one steals, the spawn has very low overhead, and the worker proceeds at full speed. If a steal *does* happen, it incurs the cost of a thief taking the continuation, which includes synchronizing on the deque, etc., but steals are rare enough that overall efficiency remains high[\[14\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=The%20work,overhead%20than%20the%20slow%20clone).

**Fairness and Load Balance in Cilk:** Cilk’s work-stealing scheduler was proven *efficient and “fair” in workload distribution* in the sense that it achieves near-linear speedups without starving any processor[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with)[\[4\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=contention%20that%20negatively%20impact%20performance,such%20as%20those%20built%20with). Each idle processor will eventually steal some continuation and do useful work. Over the entire execution, the total number of steals is bounded by a small multiple of *P · T\<sub\>∞\</sub\>* (processors times critical path length), meaning overhead grows modestly with more processors[\[9\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=the%20stealing%20phase%20when%20an,on%20P%20processors%2C%20and%20also). In practical terms, Cilk’s random steals and work-first policy yield an *even spread of work*: all threads work until the global computation is done, and none remains idle significantly earlier than others. Cilk’s theoretical guarantee (execution time ≈ T\<sub\>1\</sub\>/P \+ lower-order terms) implies each processor gets roughly T\<sub\>1\</sub\>/P work – a strong form of load-balancing fairness[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with). Memory (space) usage is also kept fair and efficient: the work-first policy ensures that *stack space* usage per worker is bounded by the sequential depth of that worker’s current task. In fact, an important benefit of work-first scheduling is a guarantee that the parallel execution uses at most a constant factor more stack space than a serial execution[\[15\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=An%20important%20theoretical%20advantage%20of,first%20policy). This prevents any one worker from unfairly consuming excessive memory. The Cilk-5 paper explicitly notes this space guarantee: *“the work-first policy guarantees that the space used...is bounded by a constant factor of the space used in the sequential version”*[\[15\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=An%20important%20theoretical%20advantage%20of,first%20policy).

However, it’s worth noting that Cilk’s notion of fairness is **throughput-oriented**. It does not ensure *strict fairness in task ordering or latency* for individual tasks (Cilk programs are usually written as one big computation rather than many independent tasks). Cilk uses a **depth-first (LIFO)** execution order for spawned tasks, which maximizes cache locality but can delay older tasks. If a Cilk worker keeps spawning new tasks, it will dive into the newest task each time (stack-like behavior), potentially postponing the execution of earlier spawned tasks until a thief steals them or until the worker eventually works back through its deque. This is acceptable because all tasks are part of one job and the goal is to finish the entire job quickly. It might mean some spawned subtasks don’t begin until near the end (if no steal happens), but since there’s no external timing requirement on individual tasks in a pure fork–join computation, this is not considered starvation – it’s just depth-first order. In summary, **Cilk’s scheduler prioritizes overall work completion over per-task response fairness**[\[16\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,individual%20tasks%20does%20not%20matter). As long as processors are busy and total work is done ASAP, it’s deemed a success, even if some tasks waited longer internally.

## Help-First Scheduling (Continuation-Stealing vs. Child-Stealing)

An alternative scheduling approach is **help-first**, essentially the opposite policy of work-first. Under help-first scheduling, when a task spawns a new child, the worker **continues executing the parent’s continuation (helping to finish the current task) and lets the new child task be stolen**[\[13\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=4%20Work,help%20from%20its%20peer%20workers). (This is also called *“child-stealing”*, since idle workers steal the spawned child tasks rather than the continuations.) The name “help-first” implies that the spawning worker *asks for help* on the new task before doing it itself – it helps finish its own current work first while leaving the new subtask for others[\[17\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=continuation%20to%20be%20stolen%20by,help%20from%20its%20peer%20workers).

**Trade-offs:** Help-first vs. work-first is a fundamental scheduling decision with implications for fairness and performance: \- **Work-First (Cilk-style, continuation-stealing):** \- *Overhead:* Minimizes creation overhead on the current path (fast local execution). Ideal when steals are rare[\[14\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=The%20work,overhead%20than%20the%20slow%20clone). \- *Memory:* Bounded space usage (each worker’s call stack resembles a sequential stack most of the time)[\[15\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=An%20important%20theoretical%20advantage%20of,first%20policy). \- *Load Balance:* Steals happen if needed; each steal takes a continuation (which might encompass a lot of remaining work). If many tasks are spawned in quick succession, steals will occur **sequentially** – e.g. one worker pushing multiple continuations requires idle workers to steal them one by one. This can serialize part of the load distribution process[\[18\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=outer%20loop%20be%20T%2C%20the,According)[\[19\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=continuation%20for%20each%20iteration%20be,acquire%20the%20lock%20on%20the). \- *Fairness:* Biased toward throughput and locality. Older tasks (continuations) might sit in deques until stolen, which could be considered *unfair* if those tasks correspond to other “agents” waiting. But for monolithic jobs, this is fine.

* **Help-First (child-stealing):**

* *Overhead:* The spawning worker must create a separate task object for the child immediately (so the child can be stolen). It essentially offloads new work to the deque, incurring overhead on the current thread each spawn. If steals are infrequent, this overhead is wasted effort (since the worker could have just done the work itself more cheaply). Thus help-first can be less efficient when tasks are small or steals rare[\[10\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=predicted%20by%20past%20work%20%28e,).

* *Memory:* Potentially larger stack space usage – since the spawning worker keeps its call stack for the parent (which can become deep if it keeps spawning and continuing), and child tasks allocate their own stacks when stolen or executed later. Theoretical space bounds are harder to guarantee[\[15\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=An%20important%20theoretical%20advantage%20of,first%20policy)[\[20\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L743%20policy%20to,3%5D%20which%20is). In extreme cases (deep recursion), work-first avoids stack overflow by turning recursion into parallel steals of continuations, whereas help-first could replicate the deep call structure on one thread’s stack. (The X10/Cilk literature notes that a purely help-first policy can risk stack overflow in depth-first recursive algorithms unless mitigated[\[21\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=if%20only%20a%20work,However).)

* *Load Balance:* Steals happen on child tasks, which are created as *independent ready tasks* immediately. This has a major advantage in **parallelism and fairness of distribution**: if one worker spawns many children, it can put each child in the deque right away. Then *multiple idle workers can steal different children concurrently*, **parallelizing the work distribution**. There’s no single serial bottleneck of one thief at a time taking continuations[\[22\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L607%20for%20the,free)[\[18\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=outer%20loop%20be%20T%2C%20the,According). In other words, help-first can more rapidly spread out a bulk of tasks to many workers. This tends to improve *load-balance fairness* when there is a surge of ready tasks. The scheduling overhead of steals can also be lower on average in such cases, because steal attempts can happen in parallel without contending on the same deque lock repeatedly[\[22\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L607%20for%20the,free).

* *Fairness:* Because the original thread continues its own work, it doesn’t bias toward the newest task. In fact, help-first is more **breadth-first**: a spawner will finish what it was doing, so tasks get started more in the order they were generated. No task gets buried deep in one thread’s call stack as easily – new tasks are immediately available to others. This can reduce starvation of older tasks and can be viewed as *fairer to each spawned task’s start time*. It “shares” the work faster among peers, which is beneficial in *agent-based scenarios* where each spawned task might represent an independent agent’s work that shouldn’t be delayed behind the spawner’s own agenda.

**Cilk vs. Help-First:** Classic Cilk strictly used the work-first policy, but researchers have explored help-first scheduling in other systems (X10, Habanero-Java, etc.)[\[23\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=as%20embodied%20in%20Cilk%E2%80%99s%20implementation,Performance%20results%20on%20two%20different)[\[24\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=scheduler%20with%20compiler%20support%20for,first%20policy%20and%20vice%20versa). Notably, *Guo et al.* (2009) implemented a work-stealing scheduler that can operate in either work-first or help-first mode for X10’s tasking model[\[23\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=as%20embodied%20in%20Cilk%E2%80%99s%20implementation,Performance%20results%20on%20two%20different)[\[25\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=4%20Work,spawned%20task%20and%20leaves%20the). Their findings highlight the complementary strengths of each policy. For example, in a parallel loop where one iteration spawns tasks for other iterations (as in a wavefront pattern), a pure work-first Cilk-style scheduler suffered performance degradation beyond 16 threads because each additional worker had to steal a continuation one after another, incurring sequential steal overhead for P–1 steals[\[26\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=outer%20loop%20be%20T%2C%20the,operation%20on%20the%20deque%20is)[\[27\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=require%20P%E2%88%921%20steals%20and%20these,tsteal%20%E2%88%97%20P). The help-first mode, by contrast, allowed all those loop iterations to be stolen and executed in parallel, scaling well up to 64 threads for that benchmark[\[28\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=inner%20loop,to%20migrate%20a%20task%20from)[\[29\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=average%2C%20work,thread%20UltraSPARC). In the *SOR* (Successive Over-Relaxation) benchmark with nested parallel loops, help-first scheduling achieved higher speedups on large core counts, whereas work-first plateaued or slowed when steal overhead dominated at high P[\[28\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=inner%20loop,to%20migrate%20a%20task%20from)[\[18\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=outer%20loop%20be%20T%2C%20the,According). Similarly, for other loop-parallel codes (like the NAS CG and LUFact benchmarks), help-first outperformed work-first by a large margin on many-core machines[\[29\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=average%2C%20work,thread%20UltraSPARC)[\[30\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=the%20JGF%20and%20NPB%20benchmarks,In%20summary%2C%20the%20performance). These cases involve plentiful, parallel tasks where rapid distribution (and thus fairness in load assignment) is more critical than minimizing single-task spawn overhead.

On the other hand, help-first was not universally better. For fine-grained recursive computations (e.g. a Fibonacci computation with very small tasks), the work-first policy significantly outperformed help-first[\[10\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=predicted%20by%20past%20work%20%28e,). With small tasks, steals are rare relative to the number of spawns, so work-first’s lower spawn overhead pays off. The help-first policy in such cases added unnecessary overhead by creating stealable tasks that mostly weren’t stolen, resulting in slower execution[\[10\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=predicted%20by%20past%20work%20%28e,). The X10 scheduler experiments confirmed that **work-first excels for algorithms with small or moderate task granularity**, whereas **help-first shines for programs with massive parallelism or irregular parallel structures** (like irregular graphs or very deep task trees) where the steal rate is higher[\[31\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=predicted%20by%20past%20work%20%28e,)[\[29\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=average%2C%20work,thread%20UltraSPARC). In fact, their combined work-stealing framework could even dynamically switch policies or use heuristics (e.g., a *threshold* to decide when to treat a spawn in work-first vs. help-first manner) to get the best of both[\[20\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L743%20policy%20to,3%5D%20which%20is)[\[32\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=optimization%20for%20adaptive%20batching%20of,to%20the%20XWS%20library%20so). This hints at an adaptive approach to fairness: use help-first when a flood of tasks needs broad distribution (improving fairness/load balance across threads), and use work-first when tasks are few or tiny (minimizing overhead and respecting locality).

### Fairness in Dynamic Agent Task Environments

The concept of fairness becomes more visible in **dynamic agent-based task environments** – imagine an actor system or a server handling many independent user requests (agents), each represented by tasks. Here fairness means *each agent’s tasks should get a fair share of processing and not be starved by others*. Traditional Cilk-style work-stealing, as noted, is **work-centric**: it cares about keeping CPUs busy and finishing total work quickly, but it may cause latency disparity for individual tasks[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks). For example, if one worker continually spawns new tasks (for one agent or a subset of work), that worker will keep working on the newest task (LIFO order), and older tasks (perhaps belonging to other agents or earlier requests) might sit in the deque. They will eventually be stolen or executed, but possibly after a delay. In an interactive setting, this is undesirable – you don’t want a request that arrived first to always be delayed behind later requests.

**ForkJoinPool Fairness Modes:** The Java ForkJoinPool addresses this to some extent by offering an **“async mode”** option. By default, ForkJoinPool uses *non-async mode*, which is **LIFO** for local tasks (the just-forked task is executed next by the worker)[\[34\]](https://www.reddit.com/r/java/comments/5zel60/whats_with_the_forkjoinpoolcommonpool_default/#:~:text=What%27s%20with%20the%20ForkJoinPool,as%20that%20is%20the)[\[35\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20). This mirrors Cilk’s work-first behavior, optimizing throughput. However, if you create a ForkJoinPool with asyncMode=true, the worker threads will use **FIFO order** locally – meaning each worker will execute tasks in the order they were added (older tasks first)[\[34\]](https://www.reddit.com/r/java/comments/5zel60/whats_with_the_forkjoinpoolcommonpool_default/#:~:text=What%27s%20with%20the%20ForkJoinPool,as%20that%20is%20the)[\[36\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20). This obviously sacrifices some locality and throughput, but it provides a more fair scheduling for **“event-style” or agent-style tasks that are never joined**[\[37\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=virtue%20of%20employing%20work,tasks%20that%20are%20never%20joined). In such cases (e.g., a pool processing incoming events or messages), FIFO ensures no task sits unduly long just because the thread that picked it up keeps grabbing new ones. The ForkJoinPool Javadoc notes that **asyncMode is appropriate for event-driven tasks that should be processed in submission order**, whereas the default LIFO mode is tuned for fork/join computations that benefit from depth-first processing[\[37\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=virtue%20of%20employing%20work,tasks%20that%20are%20never%20joined).

Even without enabling asyncMode, the ForkJoinPool strives for fairness at the *pool level* by design. If one worker’s deque grows long (meaning it has a backlog of tasks, possibly from many agents), other workers will steal those tasks from the front. This prevents one thread from hoarding tasks while others are idle. The pool provides metrics like getStealCount() so you can monitor this behavior: **ideally, “steal counts should be high enough to keep threads busy, but low enough to avoid excessive contention”**[\[38\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=public%C2%A0long%C2%A0getStealCount). A healthy balance indicates the work is evenly spread – enough stealing happened that no thread was idle or starved, but not so much that threads spent too much time stealing instead of doing work[\[38\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=public%C2%A0long%C2%A0getStealCount). In essence, *load-balance fairness* is measured by how evenly the work (and time) is divided among threads; a roughly equal number of tasks processed or time spent per worker is a good sign. ForkJoinPool’s toString() even reports per-worker queue sizes and steal counts to help identify imbalance[\[39\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=status%20check%20methods%20,convenient%20form%20for%20informal%20monitoring)[\[40\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=).

For *task fairness* (each task or agent gets timely attention), aside from FIFO mode, the ForkJoin framework relies on cooperative techniques. One such technique is **task joining and help**: when a worker performs a join() on a forked task that isn’t finished, the worker will **“help” execute other tasks** (possibly by stealing from others) while waiting[\[41\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Looks%20the%20same%2C%20does%20it,CPU)[\[42\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=around%20for%20other%20tasks%20that,the%20face%20of%20blocking%20actions). This *“helping while waiting”* avoids a worker sitting idle and ensures that if an agent’s task is blocked waiting for another, the thread will go do something else useful (maybe help another agent’s task) – indirectly promoting fairness by not letting any core go to waste. Also, if a task (say an agent’s action) spawns subtasks, the thread might switch to another if it can’t proceed, again ensuring progress is made somewhere. These mechanisms improve fairness in terms of throughput (no idle cores) and also reduce latency for others by utilizing would-be idle time.

It’s important to realize that **in purely dynamic agent systems (like actor frameworks)**, additional measures are often needed for strict fairness. For example, an actor runtime built on work-stealing might implement *preemption or time-slicing* to ensure no single actor hogs a thread[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks)[\[43\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=are%20created%20that%20run%20from,parallel%20workload.%20This). Since work-stealing is non-preemptive (threads run tasks to completion unless they explicitly yield), an actor could in theory keep a worker busy indefinitely. Frameworks like Akka or Erlang address this by breaking long-running actor messages into smaller tasks or by cooperative yields. The Waterloo research on *fair scheduling for actor systems* notes that actor applications demand fairness and low tail-latency for individual tasks, unlike batch fork/join jobs[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks). They found that standard work-stealing, while efficient, might produce long latency tails if some actors keep generating work – essentially because default work-stealing will favor that busy actor’s locality (LIFO) and delay servicing other actors’ messages[\[44\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=model%2C%20such%20as%20a%20chat,start%20to%20end%20without%20yielding). The solution is often to incorporate **fair queueing** on a per-actor basis atop the work-stealing or to tweak the stealing strategy to consider task age or source. For instance, an actor scheduler could steal the oldest task in the system (not just oldest in one deque) or use a **round-robin stealing** policy among actors to improve fairness. Some implementations introduce **priorities or task budgets**: e.g., each task (agent request) might carry a timestamp or priority, and workers might occasionally prefer stealing older tasks to give them CPU time (preventing starvation). These are advanced strategies beyond vanilla Cilk or ForkJoin, but they highlight that *fairness in dynamic multi-agent contexts may require layering scheduling policies on top of raw work-stealing*[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks)[\[43\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=are%20created%20that%20run%20from,parallel%20workload.%20This).

## Java ForkJoinPool Work-Stealing and Load Balancing

Java’s ForkJoinPool (FJP), introduced in Java 7, was heavily influenced by Cilk’s algorithm and uses the work-stealing pattern to implement the ExecutorService interface for parallel tasks. Each worker thread in FJP has a deque (implemented as a ForkJoinPool.WorkQueue). By default it operates in the **same style as Cilk’s work-first scheduler**: when you call fork() on a ForkJoinTask, the worker pushes the new task to the bottom of its deque and *continues executing the current task*, effectively treating the fork as a spawn whose continuation might be stolen[\[45\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=uses%20a%20work,threads%20that%20are%20still%20busy)[\[46\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=We%20have%20two%20tasks%20,steal%20d%20from%20thread%202). If the worker later calls join() on that task, and it’s not done, the worker will try to pop another task (maybe the one it just forked, if no one stole it) or steal from others – this is the quiescence mechanism to avoid true blocking[\[41\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Looks%20the%20same%2C%20does%20it,CPU)[\[47\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=around%20for%20other%20tasks%20that,the%20face%20of%20blocking%20actions). This design yields the same benefits as Cilk: excellent throughput and theoretical efficiency. ForkJoinPool also caps the number of threads to the parallelism level (usually number of cores) to avoid oversubscription, and it manages an internal *compensating threads* feature: if all threads are busy but one is blocked (e.g., waiting on I/O via a ManagedBlocker), it may spawn a temporary extra thread to keep the CPU busy[\[12\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=However%20there%20is%20a%20trap%3A,s). This ensures load balance even when some threads can’t progress (a form of fairness to the overall system throughput).

**Deque implementation:** The FJP uses an efficient array-based deque. Doug Lea’s implementation was initially lock-based for steals (each queue had a lock or at least a CAS on a status flag when stealing)[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task). Later versions improved on this. Still, as noted in research, **the Fork/Join framework uses a variant of Cilk’s deque protocol but requires the thief to acquire a lock on the victim’s deque** (at least in the original algorithm)[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task). In practice, the locking is minimal (only during steal, not on push/pop), and as each steal grabs one task, contention is low. The overhead of an occasional lock on steal is offset by the benefit of work redistribution.

**Load-balancing metrics:** The ForkJoinPool provides methods like getStealCount() and getQueuedTaskCount() for introspection[\[40\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=)[\[48\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=%2A%20). A high steal count indicates that many tasks have been rebalanced among threads – which is good to a point (it means idle threads found work), but if it’s extremely high, it could signal tasks are too fine-grained or distributed in a cache-unfriendly way[\[38\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=public%C2%A0long%C2%A0getStealCount). Ideally, we want *enough* stealing that no thread stays idle (fair use of all CPUs) but not so much that overhead dominates. The FJP also has a getActiveThreadCount() and getPoolSize() to see how many threads are actually busy versus created[\[49\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=match%20at%20L726%20Returns%20an,the%20number%20of%20active%20threads)[\[50\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Returns%20an%20estimate%20of%20the,the%20number%20of%20active%20threads). In a perfectly balanced scenario, all threads are active doing roughly equal amounts of work until near the end of the computation. If one thread finishes much earlier (with nothing to steal), that indicates an imbalance – perhaps one thread got a disproportionate share of lightweight tasks, or tasks were not divisible enough. The work-stealing strategy aims to minimize this by letting that thread steal from others. In fact, a classic anecdotal metric is *“time to idle”*: how soon does the first thread go idle? The closer that is to the overall completion time, the better the load balance. In fork–join, ideally no thread goes idle until the last tasks are nearly done (all threads finish around the same time).

**Fairness considerations:** Out-of-the-box, ForkJoinPool’s goal is maximizing parallel throughput rather than strict fairness per task. It will, for example, allow one busy worker to keep executing a stream of tasks in LIFO order, which could starve its older tasks from running on that same thread. However, because other threads steal those older tasks, starvation is avoided – eventually every task gets run by some thread. There’s no central scheduler reordering tasks based on age or origin; it’s purely driven by local deque order and steal availability. In practice, for pure computations, this yields excellent utilization. For more *fair scheduling*, as discussed, FJP offers the **asyncMode (FIFO)** option[\[35\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20). This is particularly useful if the tasks are more like independent events (like a bunch of Runnables submitted to the pool) and you care about servicing them in arrival order. For example, if using ForkJoinPool.commonPool() (which by default is LIFO) to process tasks from different clients, a flood of new tasks could continuously delay earlier ones on the same thread. By toggling to FIFO mode, you ensure a given worker will finish what it queued first before newer arrivals, at some cost to cache locality[\[34\]](https://www.reddit.com/r/java/comments/5zel60/whats_with_the_forkjoinpoolcommonpool_default/#:~:text=What%27s%20with%20the%20ForkJoinPool,as%20that%20is%20the).

It’s also possible to simulate priority or fairness by using multiple pools or custom scheduling. For instance, one could dedicate certain threads or pools to certain types of agents or periodically signal tasks to yield (though ForkJoinTask doesn’t naturally yield, one could break tasks into smaller chunks). However, those are beyond the default design.

## Comparison of Cilk and Java ForkJoinPool Scheduling

The table below summarizes key differences and similarities between Cilk’s scheduler and the Java ForkJoinPool, with an emphasis on scheduling policies, fairness, and load balancing features:

| Aspect | Cilk (Work-First) | Java ForkJoinPool |
| :---- | :---- | :---- |
| **Scheduling Policy** | *Work-First (Continuation-Stealing):* By default, executes the spawned task immediately, pushing the parent continuation onto the deque[\[13\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=4%20Work,help%20from%20its%20peer%20workers). This exploits the *work-first principle* for efficiency. Cilk doesn’t natively implement help-first in released versions, though conceptually Cilk \= work-first. | *Work-Stealing, Work-First by default:* By design, behaves similarly to Cilk’s policy – new tasks are forked and the forker continues with its current work, effectively LIFO scheduling for local tasks[\[1\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=these%20in%20a%20random%20order,form%20of%20hierarchy%20without%20balancing)[\[45\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=uses%20a%20work,threads%20that%20are%20still%20busy). However, FJP can be configured for FIFO (help-first-like) local processing via asyncMode=true for fairness[\[35\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20). |
| **Deque Design** | Each worker has a double-ended queue. Cilk’s algorithm uses a lock-free push/pop for the owner and a locking (or atomic) steal from the opposite end[\[8\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=to%20the%20THE%20protocol%20,If%20the%20asyncs%20are%20scheduled)[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task). The Chase–Lev circular deque or similar is employed in modern incarnations. Space-efficient: Continuations (parent frames) are stored in deque; the worker’s call stack holds the currently executing path, ensuring bounded memory use[\[15\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=An%20important%20theoretical%20advantage%20of,first%20policy). | Each worker in ForkJoinPool has a deque (WorkQueue). Owner pushes/pops bottom without locks; thieves CAS the top to steal. The implementation is derived from Chase–Lev’s lock-free deque (in Java 8+ it’s highly optimized with careful memory ordering). A thief might briefly lock a victim’s queue in some versions[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task). The deque size expands dynamically. FJP also maintains a **common submission queue** for external submissions (to distribute externally submitted tasks to workers fairly). |
| **Work Distribution** | **Random work stealing:** Idle workers pick victims uniformly at random[\[9\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=the%20stealing%20phase%20when%20an,on%20P%20processors%2C%20and%20also). This avoids centralized scheduling and balances load probabilistically. Cilk’s theoretical bounds show efficient distribution (each processor does \~T\<sub\>1\</sub\>/P work)[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with). Steals are infrequent in well-structured fork–join jobs, keeping overhead low[\[14\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=The%20work,overhead%20than%20the%20slow%20clone). | **Random (or roving) steals:** ForkJoinPool workers also attempt to steal from random victims when idle[\[9\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=the%20stealing%20phase%20when%20an,on%20P%20processors%2C%20and%20also). This decentralization yields robust load balance. The ForkJoinPool also includes internal logic to maintain parallelism: if a worker is waiting to join a task, it will help steal tasks (help join) rather than truly block[\[41\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Looks%20the%20same%2C%20does%20it,CPU). Additionally, the pool can spawn extra threads if some threads block on external waits[\[12\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=However%20there%20is%20a%20trap%3A,s). These ensure no cores sit idle (good load balance) even in complex scenarios. |
| **Fairness (Workers)** | **Load-Balancing Fairness:** All workers get work as needed through steals, preventing any one worker from being idle while others overload. Cilk’s scheduler guarantees *work fairness* in that no processor stays idle if there is work available anywhere[\[2\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Fork%2Fjoin%20is%20different%20from%20a,From%20Fork%2FJoin). Over a run, each worker executes roughly an equal portion of the total work (differences bounded by the critical path overhead)[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with). There is no centralized control, but fairness emerges from random stealing. **Task Ordering Fairness:** Cilk’s default strategy is not FIFO; it’s depth-first. A worker may delay running some tasks (continuations) if it keeps spawning new ones. Thus, **it does not provide fairness in service order** – it optimizes completion time, not per-task wait time[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks). This is acceptable for parallel computations where all tasks are parts of one whole. | **Load-Balancing Fairness:** Similarly, FJP ensures all threads are utilized. The steal count metric can be used to verify that work migrated to idle threads[\[38\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=public%C2%A0long%C2%A0getStealCount). The pool tries to keep threads within one task of each other in terms of load, insofar as random stealing evens it out. There’s no starvation of any thread – any idle thread will promptly grab work if any exists. **Task Service Fairness:** By default, FJP is also *LIFO-biased*, so it doesn’t guarantee FIFO servicing of tasks. However, it allows an **optional FIFO mode** for fairness[\[35\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20), useful in event-driven uses. In FIFO (async) mode, workers process tasks in submission order, improving fairness/latency at some throughput cost. In addition, multiple design features (helping on join, compensation threads) ensure that if one task is waiting or blocked, others progress – maintaining fairness in throughput and avoiding deadlocks[\[41\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Looks%20the%20same%2C%20does%20it,CPU)[\[12\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=However%20there%20is%20a%20trap%3A,s). |
| **Help-First Capability** | Cilk itself (the language/runtime) sticks to work-first. But research variants (Cilk derivatives, X10, etc.) have implemented help-first scheduling and even *adaptive hybrid* policies. For example, an X10 work-stealing runtime could switch to help-first for certain async tasks, enabling parallel steals and reducing contention[\[22\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L607%20for%20the,free)[\[20\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L743%20policy%20to,3%5D%20which%20is). These are not part of classic Cilk but show how Cilk’s model can be extended for fairness or performance in specific cases. | ForkJoinPool intrinsically supports a form of help-first in the sense that if tasks are never joined (independent tasks), using FIFO mode effectively means child tasks are left for others while the thread continues with older tasks (like help-first semantics). Also, tasks forked by an *external* thread (via execute) are placed into a random worker’s deque or a submission queue, from which workers steal – so the submitting thread is “asking for help” immediately. In work-stealing literature, FJP is noted to implement “a variant of Cilk’s THE protocol” and can accommodate help-first scheduling with modifications[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task), though by default it remains work-first oriented. |

**Table:** Comparison of Cilk vs. ForkJoinPool scheduling and fairness mechanisms.

## Fairness and Load Balance Measurements

How do we measure and confirm fairness and load balance in such systems? Aside from qualitative behavior, researchers and practitioners use several metrics: \- **Speedup and Utilization:** A primary indicator of load balance is the achieved speedup vs. ideal. If Cilk or ForkJoin yields near-linear speedup on P processors for a given task, it means work was evenly spread with minimal idle time. For instance, Cilk has shown linear speedups on many benchmarks, and the X10 work-stealing scheduler achieved up to 22.8× speedup on 64 threads (versus a single thread) for certain loop-parallel benchmarks[\[51\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=the%20JGF%20and%20NPB%20benchmarks,sharing%20scheduler%20for%20X10). When work distribution was poor (like Cilk work-first on SOR beyond 16 cores), the speedup curve flattens or drops[\[28\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=inner%20loop,to%20migrate%20a%20task%20from)[\[18\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=outer%20loop%20be%20T%2C%20the,According), revealing a load imbalance due to serialized steals. \- **Steal Count and Task Throughput:** As mentioned, frameworks expose steal counts. One can also look at the total number of tasks processed per thread. Ideally, these numbers are roughly equal. If one thread processed 1000 tasks and another 100, there was imbalance (perhaps tasks were unequal in cost or stealing didn’t even it out). Fairness would mean each thread did similar work units. In a perfectly balanced scenario, steals help redistribute tasks such that towards the end, all deques are near empty at the same time. High variance in queue length over time indicates imbalance. Tools that visualize the work distribution (time each thread is busy vs idle) often show a Gantt chart of threads – a fair scheduling will show all threads busy until almost the end. \- **Latency or Wait Time Distribution:** In agent systems, one can measure how long tasks wait in the queue before being executed. A fair scheduler (with FIFO or other fairness) would have a narrower distribution of wait times. A pure LIFO work-stealing might show some tasks that wait much longer (if they were pushed to bottom and never stolen for a while). For example, a study on *responsive parallelism* introduced metrics like *max lag* or *tail latency* for tasks[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks)[\[44\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=model%2C%20such%20as%20a%20chat,start%20to%20end%20without%20yielding). Ensuring that no task waits excessively compared to others (especially tasks from different agents) is a measure of fairness. Work-stealing without adjustments can suffer from long tail latencies for some tasks in such scenarios, whereas adding fairness policies (e.g. time-slicing or FIFO queues) reduces the worst-case wait. \- **Fairness Index:** Sometimes a statistical metric like Jain’s Fairness Index can be applied to the work done by threads or the service rate per agent. If we consider each worker’s executed work as a “share”, we want those shares equal. If considering each agent’s tasks, we want each agent to get equal progress over time. Work-stealing inherently focuses on equalizing workers’ load, not per-agent service, so additional layers (like mapping agents to specific tasks or priorities) may be needed to measure fairness per agent.

In summary, **Cilk and ForkJoinPool both excel at *dynamic load balancing***: they keep all processors busy by letting idle ones steal work, without any central scheduling bottleneck. Cilk’s work-stealing is tuned for maximum efficiency and theoretical optimality in fully-strict fork–join computations, using the work-first policy to minimize overhead and guarantee space bounds[\[15\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=An%20important%20theoretical%20advantage%20of,first%20policy). Java’s ForkJoinPool adopts these ideas and adds pragmatic enhancements (like managed blockers and an optional fairness mode) to handle more diverse usage patterns. **Fairness in terms of equal load is largely achieved by random stealing**, which in expectation distributes work evenly. **Fairness in terms of task servicing order**, however, differs by context: Cilk’s default depth-first approach favors throughput and can delay some tasks (acceptable in HPC workloads), whereas Java’s ForkJoinPool acknowledges the need for fairness in more asynchronous, agent-oriented scenarios by offering FIFO mode and encouraging small, non-blocking tasks that yield often. For truly *dynamic agent systems* (actors, etc.), pure work-stealing may need augmentation (e.g. priority steals, work-sharing, or time-slicing) to meet fairness goals[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks)[\[43\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=are%20created%20that%20run%20from,parallel%20workload.%20This), but the core strength of work-stealing – **no centralized scheduler and adaptive load balance** – remains a foundation that these frameworks build upon for scalable performance.

**Sources:**

* Blumofe, R. et al. “Cilk: An Efficient Multithreaded Runtime System.” *Journal of Parallel and Distributed Computing*, 1995\. (Describes Cilk’s work-stealing and work-first principle)

* Guo, Y. et al. “Work-First and Help-First Scheduling Policies for Async-Finish Task Parallelism.” *IPDPS 2009*. (Comparative study of work-first vs help-first in X10)[\[13\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=4%20Work,help%20from%20its%20peer%20workers)[\[14\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=The%20work,overhead%20than%20the%20slow%20clone)[\[29\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=average%2C%20work,thread%20UltraSPARC)

* **Java ForkJoinPool documentation**, Java SE 8 (Doug Lea et al.)[\[36\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20)[\[38\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=public%C2%A0long%C2%A0getStealCount)

* *StackOverflow Q\&A:* “How is the fork/join framework better than a thread pool?” – explains ForkJoinPool LIFO vs FIFO behavior[\[1\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=these%20in%20a%20random%20order,form%20of%20hierarchy%20without%20balancing)[\[2\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Fork%2Fjoin%20is%20different%20from%20a,From%20Fork%2FJoin)

* Thieves and locality: Chase & Lev, SPAA 2005, “Dynamic Circular Work-Stealing Deque” (lock-free deque algorithm used in many runtimes)[\[6\]](https://hackage.haskell.org/package/chaselev-deque#:~:text=chaselev,stealing)[\[7\]](https://blog.molecular-matters.com/2015/09/25/job-system-2-0-lock-free-work-stealing-part-3-going-lock-free/#:~:text=Musings%20blog.molecular,Steals%20a%20job)

* Waterloo UW tech report on actor scheduling fairness[\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks)[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with) (discusses fairness and latency in work-stealing schedulers for actors).

---

[\[1\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=these%20in%20a%20random%20order,form%20of%20hierarchy%20without%20balancing) [\[2\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Fork%2Fjoin%20is%20different%20from%20a,From%20Fork%2FJoin) [\[11\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=%7D%20if%20%28n%20,2%29%3B%20%7D) [\[12\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=However%20there%20is%20a%20trap%3A,s) [\[41\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=Looks%20the%20same%2C%20does%20it,CPU) [\[42\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=around%20for%20other%20tasks%20that,the%20face%20of%20blocking%20actions) [\[45\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=uses%20a%20work,threads%20that%20are%20still%20busy) [\[46\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=We%20have%20two%20tasks%20,steal%20d%20from%20thread%202) [\[47\]](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool#:~:text=around%20for%20other%20tasks%20that,the%20face%20of%20blocking%20actions) java \- How is the fork/join framework better than a thread pool? \- Stack Overflow

[https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool](https://stackoverflow.com/questions/7926864/how-is-the-fork-join-framework-better-than-a-thread-pool)

[\[3\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Multiprocessor%20scheduling%20is%20a%20well,for%20general%20multi%02threaded%20programs%20with) [\[4\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=contention%20that%20negatively%20impact%20performance,such%20as%20those%20built%20with) [\[9\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=the%20stealing%20phase%20when%20an,on%20P%20processors%2C%20and%20also) [\[16\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,individual%20tasks%20does%20not%20matter) [\[33\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=Most%20importantly%2C%20applications%20written%20using,task%20parallelism%20many%20lightweight%20tasks) [\[43\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=are%20created%20that%20run%20from,parallel%20workload.%20This) [\[44\]](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf#:~:text=model%2C%20such%20as%20a%20chat,start%20to%20end%20without%20yielding) cs.uwaterloo.ca

[https://cs.uwaterloo.ca/\~mkarsten/papers/ipdps2018-preprint.pdf](https://cs.uwaterloo.ca/~mkarsten/papers/ipdps2018-preprint.pdf)

[\[5\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=write%20divide,finish%20task) [\[8\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=to%20the%20THE%20protocol%20,If%20the%20asyncs%20are%20scheduled) [\[10\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=predicted%20by%20past%20work%20%28e,) [\[13\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=4%20Work,help%20from%20its%20peer%20workers) [\[14\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=The%20work,overhead%20than%20the%20slow%20clone) [\[15\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=An%20important%20theoretical%20advantage%20of,first%20policy) [\[17\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=continuation%20to%20be%20stolen%20by,help%20from%20its%20peer%20workers) [\[18\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=outer%20loop%20be%20T%2C%20the,According) [\[19\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=continuation%20for%20each%20iteration%20be,acquire%20the%20lock%20on%20the) [\[20\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L743%20policy%20to,3%5D%20which%20is) [\[21\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=if%20only%20a%20work,However) [\[22\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=match%20at%20L607%20for%20the,free) [\[23\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=as%20embodied%20in%20Cilk%E2%80%99s%20implementation,Performance%20results%20on%20two%20different) [\[24\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=scheduler%20with%20compiler%20support%20for,first%20policy%20and%20vice%20versa) [\[25\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=4%20Work,spawned%20task%20and%20leaves%20the) [\[26\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=outer%20loop%20be%20T%2C%20the,operation%20on%20the%20deque%20is) [\[27\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=require%20P%E2%88%921%20steals%20and%20these,tsteal%20%E2%88%97%20P) [\[28\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=inner%20loop,to%20migrate%20a%20task%20from) [\[29\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=average%2C%20work,thread%20UltraSPARC) [\[30\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=the%20JGF%20and%20NPB%20benchmarks,In%20summary%2C%20the%20performance) [\[31\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=predicted%20by%20past%20work%20%28e,) [\[32\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=optimization%20for%20adaptive%20batching%20of,to%20the%20XWS%20library%20so) [\[51\]](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf#:~:text=the%20JGF%20and%20NPB%20benchmarks,sharing%20scheduler%20for%20X10) cs.rice.edu

[https://www.cs.rice.edu/\~yguo/pubs/PID824943.pdf](https://www.cs.rice.edu/~yguo/pubs/PID824943.pdf)

[\[6\]](https://hackage.haskell.org/package/chaselev-deque#:~:text=chaselev,stealing) chaselev-deque: Chase & Lev work-stealing lock-free double-ended ...

[https://hackage.haskell.org/package/chaselev-deque](https://hackage.haskell.org/package/chaselev-deque)

[\[7\]](https://blog.molecular-matters.com/2015/09/25/job-system-2-0-lock-free-work-stealing-part-3-going-lock-free/#:~:text=Musings%20blog.molecular,Steals%20a%20job) Job System 2.0: Lock-Free Work Stealing – Part 3 \- Molecular Musings

[https://blog.molecular-matters.com/2015/09/25/job-system-2-0-lock-free-work-stealing-part-3-going-lock-free/](https://blog.molecular-matters.com/2015/09/25/job-system-2-0-lock-free-work-stealing-part-3-going-lock-free/)

[\[34\]](https://www.reddit.com/r/java/comments/5zel60/whats_with_the_forkjoinpoolcommonpool_default/#:~:text=What%27s%20with%20the%20ForkJoinPool,as%20that%20is%20the) What's with the ForkJoinPool.commonPool() default parameters?

[https://www.reddit.com/r/java/comments/5zel60/whats\_with\_the\_forkjoinpoolcommonpool\_default/](https://www.reddit.com/r/java/comments/5zel60/whats_with_the_forkjoinpoolcommonpool_default/)

[\[35\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20) [\[36\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Constructors%C2%A0%20Constructor%20Description%20) [\[37\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=virtue%20of%20employing%20work,tasks%20that%20are%20never%20joined) [\[38\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=public%C2%A0long%C2%A0getStealCount) [\[39\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=status%20check%20methods%20,convenient%20form%20for%20informal%20monitoring) [\[40\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=) [\[48\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=%2A%20) [\[49\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=match%20at%20L726%20Returns%20an,the%20number%20of%20active%20threads) [\[50\]](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#:~:text=Returns%20an%20estimate%20of%20the,the%20number%20of%20active%20threads) ForkJoinPool (Java Platform SE 8 )

[https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html)