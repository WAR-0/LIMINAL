Managing Shared State in a Tauri Turn Manager

Developing a Turn Manager that is safe under asynchronous Tauri commands and WebSocket handlers requires careful choice of concurrency pattern. Two common Rust patterns for shared, mutable state are:
	•	Using shared state with locks (e.g. Arc<Mutex<T>> or Arc<RwLock<T>>), and
	•	Using a dedicated task/actor with channels to serialize access.

Each approach has trade-offs in a high-read, low-write scenario. Below, we compare these patterns and discuss best practices for your use case.

Arc/Arc – Shared-State Concurrency

Using an Arc<Mutex<T>> or Arc<RwLock<T>> is the straightforward way to share a state object across threads/tasks in Rust. For a Turn Manager, you might keep an Arc<RwLock<TurnsState>> globally (or one per turn) and clone the Arc into each Tauri command or event handler that needs access. The read-heavy, write-light nature of your workload suggests using a read–write lock: an RwLock allows multiple concurrent readers while still ensuring exclusive access for writes ￼. This means many UI queries can check the state in parallel without blocking each other, unlike a Mutex which only permits one lock at a time. Indeed, if you used a single-threaded manager (actor), every read would queue behind the previous one – an RwLock avoids that by design ￼.

Best Practices with Arc: Keep lock sections short and never hold a lock across an .await. Acquire the lock, copy or inspect the needed data, then release it before doing any slow or blocking work (like I/O or DB writes). This prevents blocking other tasks and avoids deadlocks. If a long operation is needed based on the state, extract the data under lock and perform the operation after unlocking (this is analogous to the advice of “cloning needed data out of the RwLock” to prevent deadlocks ￼). In your case, a Tauri command can grab a read lock to get the latest turn status and immediately drop it, ensuring minimal locking time.

High Read, Low Write Optimization: The tokio RwLock is fair (FIFO) so writers won’t starve under heavy reads ￼. Given writes (state transitions) are infrequent (only on major events like PLAN_APPROVED), using a single RwLock should work well – readers won’t block each other, and occasional writes get priority to avoid starvation. If you expect different turns to be updated truly independently, you could even use finer-grained locks (e.g. one lock per turn or a concurrent map) to eliminate any false sharing. However, with only ~5 handlers total, a single lock guarding all turn states is simple and should not be a bottleneck.

Performance: In uncontended scenarios, locking overhead is negligible – a Mutex/RwLock is extremely fast (just a few atomic ops). Unlike channel messaging, locking does not require heap allocation for each operation ￼. When there’s no contention, a mutex or RwLock gives lower latency direct access to the data ￼. In fact, an informal benchmark found that directly sharing state with Arc<Mutex> was roughly 2–3× faster than an actor/message-passing approach under light load ￼. The upshot is that for a Turn Manager with low concurrency, a locking approach will be very efficient.

Correctness Considerations: Using locks means you must manually handle synchronization, but Rust’s type system prevents data races as long as you stick to thread-safe types (Send + Sync). Be cautious to avoid deadlocks if you ever have more than one lock (e.g. locking two resources in opposite order). In this simple design, you might only have one global lock, so deadlock is unlikely. Just remember not to .await while holding a lock (use an async lock like tokio::Mutex/RwLock if you absolutely must hold across .await, but even then keep it minimal). If you follow these practices, the Arc<RwLock> pattern is straightforward and easy to reason about for in-memory state.

Persistence with Locks: Since state changes must persist to SQLite, you’ll need to write to the database whenever you mutate the state. One strategy is to perform the DB update outside the lock to keep lock spans short: for example, acquire a write lock, apply the state transition in memory, clone the necessary data for persistence, then release the lock and commit that change to the DB. This introduces a tiny window where the in-memory state is updated but the DB isn’t yet – which is usually acceptable if you immediately attempt the DB write and handle errors (you can always reconcile on next startup by reloading from the DB). If that inconsistency window is a concern (for example, on crash between memory and DB update), you could reverse the order – write to DB first, then update memory – or use an atomic transaction (wrap the whole update in one operation). In practice, with infrequent writes, either approach can work. Just ensure that no two writes interleave in a way that the second one runs before the first has saved to DB. Using the lock naturally serializes writes to the in-memory state; as long as each write also commits to DB before the next write occurs, your hot state and persistent state will remain in sync.

(If you anticipate a lot of concurrent DB access or want non-blocking DB I/O, consider using a connection pool (e.g. with the deadpool crate) so that DB writes don’t block each other ￼. With only one or two writes at a time, this is optional but good for future scalability.)

Channel-Based Actor – Single-Threaded Manager Task

An alternative is to implement the Turn Manager as an actor: spawn a dedicated Tokio task that owns all turn states and have other parts of the app communicate with it via message passing (e.g. an async mpsc channel). In this model, the actor task’s event loop is the only place that mutates the state. Tauri commands or WebSocket handlers would send commands to the actor (e.g. “approve plan for Turn X”, “get status of Turn Y”), and the actor serially processes these messages. This pattern avoids explicit locks entirely – by design, only one thread (the actor) touches the state, so there’s no data race and no need for a mutex.

Design: You can structure this with an enum of message types and a handle object to send messages. For example, you might have enum TurnManagerMessage { GetState{turn_id, respond_to: oneshot::Sender<State>}, UpdateState{turn_id, new_state, ...} , ... }. The actor task (running in the background) awaits messages from an mpsc::Receiver<TurnManagerMessage> and handles each by updating the state or querying it. If a response is needed (for a query), the message can include a one-shot channel to send back data ￼ ￼. This is a common pattern for building actors in Rust with Tokio (as shown by Alice Ryhl’s Tokio actor pattern example ￼ ￼).

Advantages: The actor approach can make complex asynchronous logic easier to manage. Since all state changes funnel through one task, you naturally get serialized, atomic updates – no two writes will ever interleave or require explicit locking. This can be helpful for maintaining consistency especially when multiple things might happen at once. It’s also impossible to deadlock within the actor, because there’s no lock to accidentally hold while awaiting. As one Rust forum user noted, using a channel means “you can’t hold the lock longer than you need to” – you eliminate the possibility of awaiting while holding a mutex (and thus avoid certain deadlocks) ￼. In other words, message passing forces a discipline that prevents many synchronization bugs.

Actors shine when you have a lot of independent asynchronous tasks or external events that need to be coordinated. For example, an actor can easily incorporate timers or background triggers (since it’s an event loop, you can add a periodic wake-up or timeout check in the loop). If your turn state machine needs to, say, auto-transition after a timeout or ping an external service, the actor can handle that internally without external intervention ￼. Likewise, if you needed to manage multiple resources or connections (e.g. multiple DB connections or APIs), an actor could spawn sub-tasks or maintain a pool and distribute work – all hidden behind the one actor interface. This structured approach scales well as complexity grows; when you have many tasks and shared state, reasoning about message passing is often simpler than juggling many locks ￼. (In large systems with hundreds or thousands of tasks, using only locks can become tricky – you risk contention and deadlock as the system scales up ￼. The actor model provides a clear owner for state and a queue for work, which tends to be more robust in those cases.)

Another benefit is that you can implement backpressure or buffering strategies with an actor. For instance, you could use a bounded channel and decide what to do if it starts overflowing (drop messages, push back on senders, etc.) ￼. This is harder to do with a naked lock – if many threads try to update state at once, they’ll all pile up waiting for the lock, whereas an actor could consciously drop or coalesce redundant messages if needed. While your scenario has low concurrency, it’s good to know that an actor gives you that control if the workload increases.

Drawbacks: The primary downsides of an actor revolve around performance and complexity in a read-heavy scenario. By routing all requests (reads and writes) through a single task, you introduce a serialization point – only one message is processed at a time. This means no parallelism for readers at all ￼ ￼. In a GUI application where the UI might frequently poll or request status, an actor could become a bottleneck: each UI query has to wait its turn in the channel queue behind other messages. In contrast, with an RwLock multiple reads can occur truly concurrently. For a high-read, low-write workload, a single-threaded manager will have inherently worse throughput for reads than a well-tuned RwLock approach ￼. That said, with only a handful of threads (5 or so), this may not be a visible issue – but it’s something to consider if you expect a flurry of simultaneous UI requests.

Another cost is the overhead per message. Sending a message through a channel typically involves allocating a heap object or at least moving ownership of data into the channel. The receiving side then has to wake up and handle it. In low-contention cases, a direct mutex lock/unlock is often faster than this round-trip messaging ￼. However, the performance difference in practice might be small; one discussion notes that using a capacity-1 (bounded) channel – essentially acting as a handoff mechanism – can approach the performance of a mutex, with the main overhead being an extra data copy for the message ￼. In an asynchronous Tauri app, where the overall latency is dominated by UI rendering and possibly I/O or compute in each turn, the slight overhead of a channel likely won’t be a bottleneck. Still, the simplest possible design (locks) will have the lowest latency for each read operation.

Finally, implementing an actor means more code and complexity. You’ll need to define message types, spawn the task, and handle the lifecycle (graceful shutdown of the task when the app exits, etc.). Any function that wants to read or write state must go through the actor (usually via an async API that sends a message and awaits a response). This indirection can make the code a bit harder to follow compared to a straightforward shared struct. As one engineer put it after weighing this pattern: the mental offload of not worrying about locks/deadlocks is nice, but “the price seems high in terms of additional code, indirection, and potentially performance” ￼. For an MVP/MVT with limited complexity, an actor might be over-engineering.

Concurrency & Scaling: If at some point you needed to serve many requests truly in parallel, the actor pattern can be scaled out – for example, by spawning multiple worker tasks to handle messages (turning the channel into a work queue) ￼. But that introduces its own complexity (partitioning state or synchronizing between workers). Typically, a single actor is single-threaded. Given your Turn Manager’s concurrency is low (only a few handlers), scaling out isn’t necessary now, and the actor will effectively run single-threaded on one core for all its work. This is fine for low throughput requirements, but it’s worth noting that an actor is not magically faster – it just trades on simplicity and structured access, at the cost of parallelism on reads.

Persistence with Actor: Handling the SQLite persistence in an actor can actually be very straightforward. The actor can perform the DB write as part of handling a state-change message (for example, upon receiving a UpdateState message, first write to the DB then update the in-memory state). Because messages are processed one-by-one, you know no other state change will occur in the middle of this process. If the DB write fails, the actor can decide how to handle that (retry, report error, etc.) before updating memory. This ensures your in-memory “hot” state and the database never get out of sync – they’re updated in one sequential flow. The downside is that if the DB operation is slow, it will block the actor from processing other messages in the meantime (no other state changes or queries can be handled until it finishes). In a low-throughput scenario this is usually fine. If it became an issue, you could offload DB writes to another thread or use an async DB client; the actor can spawn a task or use an async pool to perform the query and await it. This adds complexity, but at least the actor centralizes that logic. In comparison, with the lock approach you’d likely do something similar (spawn a task for DB I/O or use an async DB driver) but have to ensure the timing is right with respect to locks. In summary, the actor model simplifies keeping persistent state in sync because the “single thread of control” ensures one thing happens at a time.

Comparison and Recommendation

Given the characteristics of your project – a handful of concurrent handlers, very frequent reads (UI status checks) and infrequent writes (state transitions) – an Arc<RwLock<...>>-based solution is likely the simplest and most efficient starting point. It allows concurrent reads which fits your high-read scenario well ￼, and it has minimal overhead and complexity. This approach is idiomatic in Rust (and Tauri): you can manage the state in the Tauri State and use an Arc to share it with commands. Just be disciplined about lock usage (short-lived locks, no awaiting while locked) and your performance and correctness will be solid. In essence, for in-memory data structures that are mostly read, “mutexes/RwLocks are better” by a common rule of thumb ￼.

The actor approach, by contrast, is beneficial if you anticipate the state management becoming more complex – for example, if you add lots of asynchronous workflows, complex sequencing of tasks, or need to react to internal events/timers. It provides a structured way to encapsulate the state machine’s logic and can prevent certain classes of errors (like forgetting to release a lock or handling simultaneous writes). However, it introduces serialization of all operations (no parallel reads) and additional code overhead. In a purely high-read scenario, a channel-based manager would likely become a single-threaded choke point with “single-threaded performance for read-heavy workloads” ￼ and no real gain to show for it.

To summarize the trade-offs:
	•	Arc Approach – Simple, direct access. Allows multiple concurrent readers (great for frequent UI polls) and one writer at a time ￼. Minimal latency and no per-operation allocation when uncontended ￼. The code is straightforward (lock, read/write, unlock). You must be careful with locking discipline, but for a single shared state, the risk of deadlock is low. Suits in-memory data that’s read often and written seldom (exactly your case) ￼.
	•	Actor (Channel) Approach – Structured, serialized access. All state changes go through one task via message-passing. Eliminates explicit locking and its pitfalls (no accidental long holds or lock-based deadlocks) ￼. Can handle more complex async patterns cleanly (internal timers, external event fan-in, backpressure). But it forces every operation (even reads) through a single queue, meaning no parallelism for readers and extra latency for each request ￼. There’s slight overhead for sending/receiving messages (though with proper design this can be quite low ￼). Code is more elaborate, but can be easier to reason about as the system grows in complexity or size ￼.

In practice, both patterns are used in Rust systems depending on needs. Given your Multiple Turn Manager context, you might even combine them: for instance, one actor per turn (so independent turns don’t block each other), or use locks for quick reads and an actor for orchestrating heavier background tasks. For the initial implementation, however, leaning on the simplicity of an Arc<RwLock> is reasonable and idiomatic. It handles the high-read, low-write scenario efficiently, and you can enforce persistence by always updating the database in the same sections where you update the in-memory state.

If you find down the road that the state management logic is getting complicated (e.g. many asynchronous triggers, or if you need to coordinate state across multiple turns atomically), that might be a signal to refactor toward an actor or async FSM pattern. Until then, using a lock is “blazingly fast” and perfectly adequate for a small number of tasks ￼, whereas the actor model is more about structuring large concurrent systems than gaining raw performance ￼ ￼.

Conclusion: For a high-read/low-write, low-concurrency Turn Manager, an Arc<RwLock>-protected state is a best-practice approach due to its simplicity and concurrency benefits for readers. Ensure each state transition holds the write lock briefly and persists to SQLite to keep the hot state in sync. The channel/actor pattern is a robust alternative that may be warranted if your logic grows more complex or if you want the peace-of-mind of single-threaded state management at the cost of throughput. In summary, “actors are better for I/O resources or complex async orchestration, while mutexes/RwLocks are better for in-memory data” ￼ – your scenario falls squarely in the latter category, so a lock-based solution should serve you well, with the actor model as an optional future enhancement.

Sources:
	•	Rust Concurrency Discussions – Trade-offs between using locks vs. channels/actors ￼ ￼
	•	Rust Forum – RwLock vs Manager (Actor) pattern for read-heavy workloads ￼
	•	Alice Ryhl, Actors with Tokio – Actor pattern implementation details ￼ ￼
	•	Reddit Rust thread – Performance comparison of Arc vs actor messaging ￼ ￼